{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e251fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MER Benchmark: CLAP embeddings + gold labels\n",
    "# ============================================\n",
    "\n",
    "# ---------- Parameters ----------\n",
    "LABELS_CSV = \"final_gold_combined.csv\"      # gold top-3 labels per song\n",
    "EMBEDS_CSV = \"clap_embeddings.csv\"    # your CLAP *audio* embeddings per song\n",
    "\n",
    "OUT_DIR       = \"mer_benchmark_outputs\"\n",
    "TEST_SIZE     = 0.20\n",
    "VAL_SIZE      = 0.10     # fraction of the remaining after test split\n",
    "RANDOM_STATE  = 42\n",
    "CALIBRATE_THRESHOLDS = True   # set False to skip per-class threshold tuning\n",
    "\n",
    "# Emotion tag vocabulary (order matters)\n",
    "TAGS = [\n",
    "    \"Stimulating\", \"Playful\", \"Soothing\", \"Sensory-Calming\",\n",
    "    \"Grounding\", \"Focusing\", \"Transitional\", \"Anxiety-Reduction\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Imports ----------\n",
    "import os, json, ast, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    f1_score, jaccard_score, accuracy_score,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb174bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilities ----------\n",
    "def ensure_outdir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_labels(labels_csv, tags):\n",
    "    df = pd.read_csv(labels_csv)\n",
    "    need = {\"filename\",\"final_top1\",\"final_top2\",\"final_top3\"}\n",
    "    missing = need - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Label CSV missing columns: {missing}\")\n",
    "\n",
    "    # multi-hot\n",
    "    Y = []\n",
    "    for _, r in df.iterrows():\n",
    "        chosen = set([str(r[\"final_top1\"]), str(r[\"final_top2\"]), str(r[\"final_top3\"])])\n",
    "        Y.append([1 if t in chosen else 0 for t in tags])\n",
    "    Y = np.array(Y, dtype=int)\n",
    "\n",
    "    # normalize filename\n",
    "    df[\"filename\"] = df[\"filename\"].astype(str).str.strip()\n",
    "    return df, Y\n",
    "\n",
    "def parse_embedding_row(row):\n",
    "    \"\"\"\n",
    "    Parse an embedding from a row that might store it as:\n",
    "    - A single 'embedding' column with a JSON / Python-like list string, or\n",
    "    - Many numeric columns (we'll select numeric cols excluding ID cols).\n",
    "    \"\"\"\n",
    "    if \"embedding\" in row.index:\n",
    "        val = row[\"embedding\"]\n",
    "        if isinstance(val, (list, np.ndarray)):\n",
    "            return np.array(val, dtype=float)\n",
    "        if isinstance(val, str):\n",
    "            try:\n",
    "                arr = ast.literal_eval(val)\n",
    "                return np.array(arr, dtype=float)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    numeric = []\n",
    "    for col, v in row.items():\n",
    "        if col.lower() in {\"filename\",\"file_name\",\"path\",\"song\",\"track\"}:\n",
    "            continue\n",
    "        try:\n",
    "            numeric.append(float(v))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return np.array(numeric, dtype=float)\n",
    "\n",
    "def load_embeddings(embeds_csv):\n",
    "    df = pd.read_csv(embeds_csv)\n",
    "    if \"file_name\" in df.columns and \"filename\" not in df.columns:\n",
    "        df = df.rename(columns={\"file_name\":\"filename\"})\n",
    "    if \"filename\" not in df.columns:\n",
    "        # try to infer\n",
    "        for c in df.columns:\n",
    "            if \"file\" in c.lower() and \"name\" in c.lower():\n",
    "                df = df.rename(columns={c:\"filename\"})\n",
    "                break\n",
    "    if \"filename\" not in df.columns:\n",
    "        raise ValueError(\"Embeddings CSV must contain a 'filename' column (or 'file_name').\")\n",
    "    df[\"filename\"] = df[\"filename\"].astype(str).str.strip()\n",
    "\n",
    "    X_rows = [parse_embedding_row(r) for _, r in df.iterrows()]\n",
    "    X = np.vstack(X_rows)\n",
    "    return df[[\"filename\"]].copy(), X\n",
    "\n",
    "def match_and_merge(labels_df, Y, embeds_df, X):\n",
    "    merged = labels_df.merge(embeds_df, on=\"filename\", how=\"inner\")\n",
    "    # align Y and X to merged order\n",
    "    idx_lut_L = {fn:i for i, fn in enumerate(labels_df[\"filename\"].tolist())}\n",
    "    idx_lut_E = {fn:i for i, fn in enumerate(embeds_df[\"filename\"].tolist())}\n",
    "    y_sel = np.vstack([Y[idx_lut_L[fn]] for fn in merged[\"filename\"]])\n",
    "    x_sel = np.vstack([X[idx_lut_E[fn]] for fn in merged[\"filename\"]])\n",
    "    return merged, x_sel, y_sel\n",
    "\n",
    "def summarize(merged_df, X, Y, tags):\n",
    "    print(f\"Total merged songs: {len(merged_df)}\")\n",
    "    print(f\"Feature matrix: {X.shape}   (n_songs, emb_dim)\")\n",
    "    print(f\"Label matrix:   {Y.shape}   (n_songs, n_tags={len(tags)})\")\n",
    "    counts = Y.sum(axis=0)\n",
    "    print(\"\\nPer-tag positive counts:\")\n",
    "    for t, c in zip(tags, counts):\n",
    "        print(f\"  {t:18s}: {int(c)}\")\n",
    "    if \"source\" in merged_df.columns:\n",
    "        print(\"\\nSource breakdown:\")\n",
    "        print(merged_df[\"source\"].value_counts())\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def multilabel_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return (\n",
    "        {\n",
    "            \"micro_f1\": float(f1_score(y_true, y_pred, average=\"micro\", zero_division=0)),\n",
    "            \"macro_f1\": float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "            \"micro_jaccard\": float(jaccard_score(y_true, y_pred, average=\"micro\", zero_division=0)),\n",
    "            \"macro_jaccard\": float(jaccard_score(y_true, y_pred, average=\"macro\", zero_division=0)),\n",
    "            \"subset_accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        },\n",
    "        y_pred,\n",
    "    )\n",
    "\n",
    "def per_class_report(y_true, y_pred, tags):\n",
    "    rows = []\n",
    "    for i, t in enumerate(tags):\n",
    "        rows.append({\n",
    "            \"tag\": t,\n",
    "            \"precision\": float(precision_score(y_true[:,i], y_pred[:,i], zero_division=0)),\n",
    "            \"recall\":    float(recall_score   (y_true[:,i], y_pred[:,i], zero_division=0)),\n",
    "            \"f1\":        float(f1_score       (y_true[:,i], y_pred[:,i], zero_division=0)),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"tag\")\n",
    "\n",
    "def topk_overlap_metrics(y_true, y_prob, k=3):\n",
    "    j_scores, pks, rks = [], [], []\n",
    "    for i in range(len(y_true)):\n",
    "        pred_idx = np.argsort(-y_prob[i])[:k]\n",
    "        pred_set = set(pred_idx.tolist())\n",
    "        gold_set = set(np.where(y_true[i]==1)[0].tolist())\n",
    "        inter = len(pred_set & gold_set)\n",
    "        union = len(pred_set | gold_set)\n",
    "        j_scores.append(inter/union if union>0 else 0.0)\n",
    "        pks.append(inter/k if k>0 else 0.0)\n",
    "        rks.append(inter/len(gold_set) if len(gold_set)>0 else 0.0)\n",
    "    return {\n",
    "        \"mean_jaccard_at_k\": float(np.mean(j_scores)),\n",
    "        \"mean_precision_at_k\": float(np.mean(pks)),\n",
    "        \"mean_recall_at_k\": float(np.mean(rks)),\n",
    "        \"k\": int(k),\n",
    "    }\n",
    "\n",
    "def calibrate_thresholds_per_class(y_true_val, y_prob_val, grid=None):\n",
    "    \"\"\"\n",
    "    Returns an array of per-class thresholds chosen to maximize F1 on validation.\n",
    "    \"\"\"\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.2, 0.8, 13)  # 0.20, 0.25, ..., 0.80\n",
    "    C = y_true_val.shape[1]\n",
    "    ths = np.full(C, 0.5, dtype=float)\n",
    "    for c in range(C):\n",
    "        best_f1, best_t = -1.0, 0.5\n",
    "        for t in grid:\n",
    "            pred = (y_prob_val[:,c] >= t).astype(int)\n",
    "            f1 = f1_score(y_true_val[:,c], pred, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        ths[c] = best_t\n",
    "    return ths\n",
    "\n",
    "def apply_thresholds(y_prob, thresholds):\n",
    "    return (y_prob >= thresholds[None, :]).astype(int)\n",
    "\n",
    "def evaluate_split(y_true, y_prob, tag_names, thresh=0.5, per_class=False):\n",
    "    metrics, y_pred = multilabel_metrics(y_true, y_prob, threshold=thresh)\n",
    "    topk = topk_overlap_metrics(y_true, y_prob, k=3)\n",
    "    out = {\"metrics\": metrics, \"topk@3\": topk}\n",
    "    if per_class:\n",
    "        out[\"per_class\"] = per_class_report(y_true, y_pred, tag_names)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load & Merge ----------\n",
    "ensure_outdir(OUT_DIR)\n",
    "\n",
    "labels_df, Y_all = load_labels(LABELS_CSV, TAGS)\n",
    "embeds_df, X_all = load_embeddings(EMBEDS_CSV)\n",
    "\n",
    "merged_df, X, Y = match_and_merge(labels_df, Y_all, embeds_df, X_all)\n",
    "summarize(merged_df, X, Y, TAGS)\n",
    "\n",
    "# ---------- Train/Val/Test split ----------\n",
    "# Stratify by final_top1 for stability (simple proxy for label distribution)\n",
    "y_primary = merged_df[\"final_top1\"].astype(str)\n",
    "\n",
    "X_trv, X_test, Y_trv, Y_test, idx_trv, idx_test = train_test_split(\n",
    "    X, Y, np.arange(len(Y)), test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_primary\n",
    ")\n",
    "\n",
    "y_primary_trv = y_primary.iloc[idx_trv]\n",
    "X_train, X_val, Y_train, Y_val, idx_train, idx_val = train_test_split(\n",
    "    X_trv, Y_trv, np.arange(len(Y_trv)),\n",
    "    test_size=VAL_SIZE/(1-TEST_SIZE), random_state=RANDOM_STATE, stratify=y_primary_trv\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit sizes → Train: {X_train.shape[0]} | Val: {X_val.shape[0]} | Test: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b4b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Standardize features ----------\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Baselines ----------\n",
    "baselines = [\n",
    "    (\"logreg_ovr\", OneVsRestClassifier(LogisticRegression(\n",
    "        solver=\"liblinear\", C=1.0, max_iter=200\n",
    "    ))),\n",
    "\n",
    "    (\"linear_svm_ovr\", OneVsRestClassifier(LinearSVC(\n",
    "        C=1.0\n",
    "    ))),  # uses decision_function; we'll map to probs via sigmoid\n",
    "\n",
    "    (\"rf_ovr\", OneVsRestClassifier(RandomForestClassifier(\n",
    "        n_estimators=400, max_depth=None, n_jobs=-1, random_state=RANDOM_STATE\n",
    "    ))),\n",
    "\n",
    "    (\"mlp_ovr\", OneVsRestClassifier(MLPClassifier(\n",
    "        hidden_layer_sizes=(512,), activation=\"relu\", alpha=1e-4,\n",
    "        max_iter=200, early_stopping=True, n_iter_no_change=10, random_state=RANDOM_STATE\n",
    "    ))),\n",
    "\n",
    "    (\"knn_ovr\", OneVsRestClassifier(KNeighborsClassifier(\n",
    "        n_neighbors=15, weights=\"distance\"\n",
    "    ))),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "ensure_outdir(os.path.join(OUT_DIR, \"per_model\"))\n",
    "\n",
    "def model_predict_proba(ovr_model, Xs):\n",
    "    \"\"\"\n",
    "    Get per-class probabilities for an OvR model, falling back to decision_function->sigmoid when needed.\n",
    "    Returns numpy array (n_samples, n_classes)\n",
    "    \"\"\"\n",
    "    # Try predict_proba (works for LR, RF, KNN, MLP in most cases)\n",
    "    if hasattr(ovr_model, \"predict_proba\"):\n",
    "        try:\n",
    "            P = ovr_model.predict_proba(Xs)\n",
    "            if isinstance(P, list):  # some OvR impls return list of arrays\n",
    "                P = np.column_stack([p[:,1] if p.ndim==2 else p for p in P])\n",
    "            return P\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback: decision_function mapped through sigmoid (for LinearSVC, etc.)\n",
    "    if hasattr(ovr_model, \"decision_function\"):\n",
    "        dec = ovr_model.decision_function(Xs)\n",
    "        if dec.ndim == 1:  # single class edge case\n",
    "            dec = dec[:, None]\n",
    "        return sigmoid(dec)\n",
    "\n",
    "    # Worst-case: use predict and cast to {0,1} \"probabilities\"\n",
    "    yp = ovr_model.predict(Xs)\n",
    "    yp = yp.astype(float)\n",
    "    return yp\n",
    "\n",
    "for name, model in baselines:\n",
    "    print(f\"\\n=== Training: {name} ===\")\n",
    "    model.fit(X_train_s, Y_train)\n",
    "\n",
    "    # Probabilities on val/test\n",
    "    Y_val_prob  = model_predict_proba(model, X_val_s)\n",
    "    Y_test_prob = model_predict_proba(model, X_test_s)\n",
    "\n",
    "    # --- Default threshold 0.5 ---\n",
    "    val_eval  = evaluate_split(Y_val,  Y_val_prob, TAGS, thresh=0.5, per_class=True)\n",
    "    test_eval = evaluate_split(Y_test, Y_test_prob, TAGS, thresh=0.5, per_class=True)\n",
    "\n",
    "    # --- Optional: per-class threshold calibration on VAL ---\n",
    "    calib = None\n",
    "    test_eval_cal = None\n",
    "    if CALIBRATE_THRESHOLDS:\n",
    "        ths = calibrate_thresholds_per_class(Y_val, Y_val_prob)\n",
    "        Y_test_pred_cal = apply_thresholds(Y_test_prob, ths)\n",
    "        # compute metrics with calibrated preds\n",
    "        metrics_cal = {\n",
    "            \"micro_f1\": float(f1_score(Y_test, Y_test_pred_cal, average=\"micro\", zero_division=0)),\n",
    "            \"macro_f1\": float(f1_score(Y_test, Y_test_pred_cal, average=\"macro\", zero_division=0)),\n",
    "            \"micro_jaccard\": float(jaccard_score(Y_test, Y_test_pred_cal, average=\"micro\", zero_division=0)),\n",
    "            \"macro_jaccard\": float(jaccard_score(Y_test, Y_test_pred_cal, average=\"macro\", zero_division=0)),\n",
    "            \"subset_accuracy\": float(accuracy_score(Y_test, Y_test_pred_cal)),\n",
    "        }\n",
    "        test_eval_cal = {\n",
    "            \"metrics\": metrics_cal,\n",
    "            \"per_class\": per_class_report(Y_test, Y_test_pred_cal, TAGS),\n",
    "            \"thresholds\": ths.tolist()\n",
    "        }\n",
    "        calib = ths.tolist()\n",
    "\n",
    "    results[name] = {\n",
    "        \"val\": {\n",
    "            \"metrics\": val_eval[\"metrics\"],\n",
    "            \"topk@3\":  val_eval[\"topk@3\"],\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"metrics\": test_eval[\"metrics\"],\n",
    "            \"topk@3\":  test_eval[\"topk@3\"],\n",
    "        },\n",
    "        \"calibrated_test\" : test_eval_cal,   # may be None\n",
    "    }\n",
    "\n",
    "    # Save per-class csvs\n",
    "    val_eval[\"per_class\"].to_csv(os.path.join(OUT_DIR, \"per_model\", f\"{name}_per_class_val.csv\"), index=False)\n",
    "    test_eval[\"per_class\"].to_csv(os.path.join(OUT_DIR, \"per_model\", f\"{name}_per_class_test.csv\"), index=False)\n",
    "    if test_eval_cal is not None:\n",
    "        test_eval_cal[\"per_class\"].to_csv(os.path.join(OUT_DIR, \"per_model\", f\"{name}_per_class_test_calibrated.csv\"), index=False)\n",
    "\n",
    "    # Print quick summary\n",
    "    print(\"  [VAL] micro F1: {:.3f} | macro F1: {:.3f} | micro J: {:.3f}\".format(\n",
    "        results[name][\"val\"][\"metrics\"][\"micro_f1\"],\n",
    "        results[name][\"val\"][\"metrics\"][\"macro_f1\"],\n",
    "        results[name][\"val\"][\"metrics\"][\"micro_jaccard\"],\n",
    "    ))\n",
    "    print(\"  [TEST] micro F1: {:.3f} | macro F1: {:.3f} | micro J: {:.3f}\".format(\n",
    "        results[name][\"test\"][\"metrics\"][\"micro_f1\"],\n",
    "        results[name][\"test\"][\"metrics\"][\"macro_f1\"],\n",
    "        results[name][\"test\"][\"metrics\"][\"micro_jaccard\"],\n",
    "    ))\n",
    "    if test_eval_cal is not None:\n",
    "        print(\"  [TEST-Calib] micro F1: {:.3f} | macro F1: {:.3f} | micro J: {:.3f}\".format(\n",
    "            test_eval_cal[\"metrics\"][\"micro_f1\"],\n",
    "            test_eval_cal[\"metrics\"][\"macro_f1\"],\n",
    "            test_eval_cal[\"metrics\"][\"micro_jaccard\"],\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17cd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- Save all metrics ----------\n",
    "# ensure_outdir(OUT_DIR)\n",
    "# with open(os.path.join(OUT_DIR, \"all_models_metrics.json\"), \"w\") as f:\n",
    "#     json.dump(results, f, indent=2)\n",
    "\n",
    "# print(\"\\nSaved all model metrics to:\", os.path.join(OUT_DIR, \"all_models_metrics.json\"))\n",
    "# ---------- Save all metrics (JSON-safe summary) ----------\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import json, os\n",
    "\n",
    "def to_builtin(x):\n",
    "    # convert numpy types to Python builtins\n",
    "    if isinstance(x, (np.integer,)):\n",
    "        return int(x)\n",
    "    if isinstance(x, (np.floating,)):\n",
    "        return float(x)\n",
    "    if isinstance(x, (np.ndarray,)):\n",
    "        return x.tolist()\n",
    "    return x\n",
    "\n",
    "serializable = {}\n",
    "for name, res in results.items():\n",
    "    entry = {\n",
    "        \"val\":   {\"metrics\": {}, \"topk@3\": {}},\n",
    "        \"test\":  {\"metrics\": {}, \"topk@3\": {}},\n",
    "    }\n",
    "    # metrics + topk@3 (already dicts of numbers)\n",
    "    for split in (\"val\", \"test\"):\n",
    "        for k, v in res[split][\"metrics\"].items():\n",
    "            entry[split][\"metrics\"][k] = to_builtin(v)\n",
    "        for k, v in res[split][\"topk@3\"].items():\n",
    "            entry[split][\"topk@3\"][k] = to_builtin(v)\n",
    "\n",
    "    # calibrated test (omit per-class DF; keep thresholds + metrics)\n",
    "    if res.get(\"calibrated_test\"):\n",
    "        cal = res[\"calibrated_test\"]\n",
    "        entry[\"calibrated_test\"] = {\n",
    "            \"metrics\": {k: to_builtin(v) for k, v in cal[\"metrics\"].items()},\n",
    "            \"thresholds\": [float(t) for t in cal.get(\"thresholds\", [])],\n",
    "        }\n",
    "\n",
    "    serializable[name] = entry\n",
    "\n",
    "ensure_outdir(OUT_DIR)\n",
    "with open(os.path.join(OUT_DIR, \"all_models_metrics.json\"), \"w\") as f:\n",
    "    json.dump(serializable, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved summary JSON to:\", os.path.join(OUT_DIR, \"all_models_metrics.json\"))\n",
    "print(\"Per-class CSVs are already saved under:\", os.path.join(OUT_DIR, \"per_model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- (Optional) Evaluate human vs CLAP-only subsets on TEST ----------\n",
    "if \"source\" in merged_df.columns:\n",
    "    print(\"\\n=== Extra: Human vs CLAP-only subsets (TEST) ===\")\n",
    "\n",
    "    test_idx_orig = idx_test  # indices into merged_df\n",
    "    test_fns = merged_df.iloc[test_idx_orig][\"filename\"].tolist()\n",
    "    fn_to_pos = {fn:i for i, fn in enumerate(merged_df[\"filename\"].tolist())}\n",
    "\n",
    "    # build a mask per subset over Y_test order\n",
    "    test_sources = merged_df.iloc[test_idx_orig][\"source\"].fillna(\"\").tolist()\n",
    "    human_mask = np.array([s.startswith(\"Human\") for s in test_sources])\n",
    "    clap_only_mask = np.array([s == \"CLAP_only\" for s in test_sources])\n",
    "\n",
    "    def subset_eval(mask, model_name, probs):\n",
    "        if not mask.any():\n",
    "            return None\n",
    "        return evaluate_split(Y_test[mask], probs[mask], TAGS, thresh=0.5, per_class=False)[\"metrics\"]\n",
    "\n",
    "    subset_summary = {}\n",
    "    for name, model in baselines:\n",
    "        # reuse probabilities computed above by re-running predict (cheap)\n",
    "        probs = model_predict_proba(model, X_test_s)\n",
    "        subset_summary[name] = {\n",
    "            \"human_subset\"    : subset_eval(human_mask,    name, probs),\n",
    "            \"clap_only_subset\": subset_eval(clap_only_mask, name, probs),\n",
    "        }\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"subset_test_metrics.json\"), \"w\") as f:\n",
    "        json.dump(subset_summary, f, indent=2)\n",
    "    print(\"Saved subset metrics to:\", os.path.join(OUT_DIR, \"subset_test_metrics.json\"))\n",
    "else:\n",
    "    print(\"\\n(no 'source' column in labels — skipping human vs CLAP-only subset eval)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c5238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

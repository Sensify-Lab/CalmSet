{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97085b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, math\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Paths\n",
    "# ----------------------------\n",
    "GOLD_PATH = \"final_gold_combined.csv\"\n",
    "DOCS_PATH = \"clap_combined.csv\"   # contains file_name + gpt_description\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load data\n",
    "# ----------------------------\n",
    "gold = pd.read_csv(GOLD_PATH)\n",
    "docs = pd.read_csv(DOCS_PATH)\n",
    "\n",
    "# Keep only the two needed columns\n",
    "docs = docs[[\"file_name\", \"gpt_description\"]].copy()\n",
    "\n",
    "# Normalize column naming across files\n",
    "# gold uses `filename`; docs uses `file_name`\n",
    "gold_docids = set(gold[\"filename\"].astype(str))\n",
    "docs[\"file_name\"] = docs[\"file_name\"].astype(str)\n",
    "\n",
    "# Sanity: ensure all docs exist\n",
    "missing_in_docs = gold_docids - set(docs[\"file_name\"])\n",
    "missing_in_gold = set(docs[\"file_name\"]) - gold_docids\n",
    "if missing_in_docs:\n",
    "    raise ValueError(f\"{len(missing_in_docs)} gold docids missing from docs file. Example: {list(missing_in_docs)[:3]}\")\n",
    "if missing_in_gold:\n",
    "    # not necessarily fatal, but should usually be zero\n",
    "    print(f\"Warning: {len(missing_in_gold)} docids in docs file not present in gold. Example: {list(missing_in_gold)[:3]}\")\n",
    "\n",
    "# Align docs to gold order (optional but nice for deterministic behavior)\n",
    "docs = docs[docs[\"file_name\"].isin(gold_docids)].drop_duplicates(\"file_name\").copy()\n",
    "docs = docs.sort_values(\"file_name\").reset_index(drop=True)\n",
    "\n",
    "assert docs.shape[0] == 432, f\"Expected 432 docs aligned to gold, got {docs.shape[0]}\"\n",
    "assert gold.shape[0] == 432, f\"Expected 432 rows in gold, got {gold.shape[0]}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27271687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) Build qrels (graded relevance from final_top1/2/3)\n",
    "# ----------------------------\n",
    "LABEL_COLS = [\"final_top1\", \"final_top2\", \"final_top3\"]\n",
    "GRADE = {\"final_top1\": 3, \"final_top2\": 2, \"final_top3\": 1}\n",
    "\n",
    "labels = sorted(pd.unique(gold[LABEL_COLS].values.ravel()))\n",
    "qrels = {lab: {} for lab in labels}\n",
    "\n",
    "for _, r in gold.iterrows():\n",
    "    docid = str(r[\"filename\"])\n",
    "    for col, rel in GRADE.items():\n",
    "        lab = r[col]\n",
    "        qrels[lab][docid] = max(qrels[lab].get(docid, 0), rel)\n",
    "\n",
    "per_query_rels = {lab: len(qrels[lab]) for lab in labels}\n",
    "print(\"Num labels (queries):\", len(labels))\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Relevant docs per query (min/max):\", min(per_query_rels.values()), max(per_query_rels.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fc970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Metrics (nDCG graded; MAP/Recall binarized rel>0)\n",
    "# ----------------------------\n",
    "def dcg_at_k(rels, k):\n",
    "    rels = rels[:k]\n",
    "    s = 0.0\n",
    "    for i, r in enumerate(rels, start=1):\n",
    "        s += (2**r - 1) / math.log2(i + 1)\n",
    "    return s\n",
    "\n",
    "def ndcg_at_k(rels, ideal_rels, k):\n",
    "    denom = dcg_at_k(ideal_rels, k)\n",
    "    return 0.0 if denom == 0 else dcg_at_k(rels, k) / denom\n",
    "\n",
    "def ap_at_k(binary_rels, k):\n",
    "    binary_rels = binary_rels[:k]\n",
    "    hits = 0\n",
    "    s = 0.0\n",
    "    for i, r in enumerate(binary_rels, start=1):\n",
    "        if r:\n",
    "            hits += 1\n",
    "            s += hits / i\n",
    "    return 0.0 if hits == 0 else s / hits\n",
    "\n",
    "def recall_at_k(binary_rels, total_relevant, k):\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return sum(binary_rels[:k]) / total_relevant\n",
    "\n",
    "def evaluate_run(ranked_docids, qrel_for_query, k=50):\n",
    "    rels = [qrel_for_query.get(d, 0) for d in ranked_docids]\n",
    "    ideal = sorted(qrel_for_query.values(), reverse=True)\n",
    "\n",
    "    ndcg = ndcg_at_k(rels, ideal, k)\n",
    "    binary = [r > 0 for r in rels]\n",
    "    ap = ap_at_k(binary, k)\n",
    "    rec = recall_at_k(binary, total_relevant=len(qrel_for_query), k=k)\n",
    "    return ndcg, ap, rec\n",
    "\n",
    "# ----------------------------\n",
    "# 4) BM25 implementation\n",
    "# ----------------------------\n",
    "def tokenize(text):\n",
    "    # Simple, robust tokenizer for descriptions\n",
    "    return re.findall(r\"[a-z0-9]+\", str(text).lower())\n",
    "\n",
    "class SimpleBM25:\n",
    "    def __init__(self, corpus_tokens, k1=1.2, b=0.75):\n",
    "        self.k1, self.b = k1, b\n",
    "        self.corpus = corpus_tokens\n",
    "        self.N = len(corpus_tokens)\n",
    "        self.doc_len = np.array([len(d) for d in corpus_tokens], dtype=float)\n",
    "        self.avgdl = float(np.mean(self.doc_len)) if self.N > 0 else 0.0\n",
    "\n",
    "        # document frequency\n",
    "        df = Counter()\n",
    "        for doc in corpus_tokens:\n",
    "            for w in set(doc):\n",
    "                df[w] += 1\n",
    "        self.df = df\n",
    "        # idf (Okapi BM25)\n",
    "        self.idf = {w: math.log(1 + (self.N - df[w] + 0.5) / (df[w] + 0.5)) for w in df}\n",
    "\n",
    "        # term frequencies per doc\n",
    "        self.tf = [Counter(doc) for doc in corpus_tokens]\n",
    "\n",
    "    def score(self, query_tokens):\n",
    "        scores = np.zeros(self.N, dtype=float)\n",
    "        for i in range(self.N):\n",
    "            dl = self.doc_len[i]\n",
    "            denom_const = self.k1 * (1 - self.b + self.b * (dl / self.avgdl if self.avgdl > 0 else 0.0))\n",
    "            tf_i = self.tf[i]\n",
    "            s = 0.0\n",
    "            for w in query_tokens:\n",
    "                if w not in self.idf:\n",
    "                    continue\n",
    "                f = tf_i.get(w, 0)\n",
    "                if f == 0:\n",
    "                    continue\n",
    "                s += self.idf[w] * (f * (self.k1 + 1)) / (f + denom_const)\n",
    "            scores[i] = s\n",
    "        return scores\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Run BM25 benchmark: label-as-query over GPT descriptions\n",
    "# ----------------------------\n",
    "docids = docs[\"file_name\"].tolist()\n",
    "corpus_tokens = [tokenize(t) for t in docs[\"gpt_description\"].fillna(\"\")]\n",
    "\n",
    "# // Create BM25 index\n",
    "bm25 = SimpleBM25(corpus_tokens, k1=1.2, b=0.75)\n",
    "\n",
    "# Default query text for each label:\n",
    "# For BM25, label strings like \"Sensory-Calming\" tokenize into [\"sensory\",\"calming\"] which works well.\n",
    "def label_to_query_text(label):\n",
    "    return label.replace(\"-\", \" \")\n",
    "\n",
    "K = 50\n",
    "rows = []\n",
    "for q in labels:\n",
    "    q_tokens = tokenize(label_to_query_text(q))\n",
    "    scores = bm25.score(q_tokens)\n",
    "\n",
    "    # Rank docs by score; tie-break by docid for determinism\n",
    "    ranked_idx = np.lexsort((np.array(docids), -scores))\n",
    "    ranked_docids = [docids[i] for i in ranked_idx]\n",
    "\n",
    "    ndcg, ap, rec = evaluate_run(ranked_docids, qrels[q], k=K)\n",
    "    rows.append((q, ndcg, ap, rec, len(qrels[q])))\n",
    "\n",
    "bm25_res = pd.DataFrame(rows, columns=[\"query_label\", f\"nDCG@{K}\", f\"MAP@{K}\", f\"Recall@{K}\", \"num_rel_docs\"])\n",
    "print(\"\\nBM25 over GPT descriptions (label-as-query):\")\n",
    "print(bm25_res.sort_values(f\"nDCG@{K}\", ascending=False).to_string(index=False))\n",
    "print(\"\\nMacro-avg (over queries):\")\n",
    "print(bm25_res[[f\"nDCG@{K}\", f\"MAP@{K}\", f\"Recall@{K}\"]].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6) Bootstrap CI over queries for BM25 nDCG@K (optional but recommended)\n",
    "# ----------------------------\n",
    "def bootstrap_ci(per_query_values, n_boot=10000, alpha=0.05, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Q = len(per_query_values)\n",
    "    boots = np.empty(n_boot, dtype=float)\n",
    "    for i in range(n_boot):\n",
    "        samp = rng.choice(per_query_values, size=Q, replace=True)\n",
    "        boots[i] = np.mean(samp)\n",
    "    lo = np.quantile(boots, alpha/2)\n",
    "    hi = np.quantile(boots, 1 - alpha/2)\n",
    "    return lo, hi\n",
    "\n",
    "lo, hi = bootstrap_ci(bm25_res[f\"nDCG@{K}\"].values, n_boot=10000, seed=42)\n",
    "print(f\"\\nBM25 macro nDCG@{K}: {bm25_res[f'nDCG@{K}'].mean():.4f}  CI95 [{lo:.4f}, {hi:.4f}]\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) (Optional) Compare BM25 vs CLAP baseline if you want both in same run\n",
    "# ----------------------------\n",
    "# If you still want to compare against CLAP label-based baseline using human_aggregated.csv:\n",
    "# - Load /mnt/data/human_aggregated.csv\n",
    "# - Build CLAP scores like before and evaluate with evaluate_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde029d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

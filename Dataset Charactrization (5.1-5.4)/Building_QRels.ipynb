{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "GOLD_PATH = \"final_gold_combined.csv\"\n",
    "HUM_PATH  = \"human_aggregated.csv\"\n",
    "\n",
    "gold = pd.read_csv(GOLD_PATH)\n",
    "hum  = pd.read_csv(HUM_PATH)\n",
    "\n",
    "# --- Canonical doc ids ---\n",
    "# We use filename as docid (stable, already in both files)\n",
    "assert gold.shape[0] == 432\n",
    "assert hum.shape[0] == 432\n",
    "assert set(gold[\"filename\"]) == set(hum[\"filename\"])\n",
    "\n",
    "# --- Label set (queries) ---\n",
    "LABEL_COLS = [\"final_top1\", \"final_top2\", \"final_top3\"]\n",
    "labels = sorted(pd.unique(gold[LABEL_COLS].values.ravel()))\n",
    "print(\"Num labels (queries):\", len(labels))\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# --- Build graded qrels: qrels[label][docid] = 3/2/1/0 ---\n",
    "GRADE = {\"final_top1\": 3, \"final_top2\": 2, \"final_top3\": 1}\n",
    "\n",
    "qrels = {lab: {} for lab in labels}\n",
    "for _, r in gold.iterrows():\n",
    "    docid = r[\"filename\"]\n",
    "    for col, rel in GRADE.items():\n",
    "        lab = r[col]\n",
    "        qrels[lab][docid] = max(qrels[lab].get(docid, 0), rel)  # safe\n",
    "\n",
    "# Per-query relevant doc counts (for reporting)\n",
    "per_query_rels = {lab: len(qrels[lab]) for lab in labels}\n",
    "print(\"Relevant docs per query (min/max):\", min(per_query_rels.values()), max(per_query_rels.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3105a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dcg_at_k(rels, k):\n",
    "    \"\"\"rels: list of relevance grades in ranked order.\"\"\"\n",
    "    rels = rels[:k]\n",
    "    s = 0.0\n",
    "    for i, r in enumerate(rels, start=1):\n",
    "        s += (2**r - 1) / math.log2(i + 1)   # graded gain\n",
    "    return s\n",
    "\n",
    "def ndcg_at_k(rels, ideal_rels, k):\n",
    "    denom = dcg_at_k(ideal_rels, k)\n",
    "    return 0.0 if denom == 0 else dcg_at_k(rels, k) / denom\n",
    "\n",
    "def ap_at_k(binary_rels, k):\n",
    "    \"\"\"Average precision uses binary relevance; standard IR practice.\"\"\"\n",
    "    binary_rels = binary_rels[:k]\n",
    "    hits = 0\n",
    "    s = 0.0\n",
    "    for i, r in enumerate(binary_rels, start=1):\n",
    "        if r:\n",
    "            hits += 1\n",
    "            s += hits / i\n",
    "    return 0.0 if hits == 0 else s / hits\n",
    "\n",
    "def recall_at_k(binary_rels, total_relevant, k):\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    return sum(binary_rels[:k]) / total_relevant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CLAP-derived scores: for each query label, score doc by CLAP rank (3/2/1/0)\n",
    "CLAP_COLS = [\"clap_top1\", \"clap_top2\", \"clap_top3\"]\n",
    "CLAP_GRADE = {\"clap_top1\": 3, \"clap_top2\": 2, \"clap_top3\": 1}\n",
    "\n",
    "clap_scores = {lab: {} for lab in labels}\n",
    "for _, r in hum.iterrows():\n",
    "    docid = r[\"filename\"]\n",
    "    for col, score in CLAP_GRADE.items():\n",
    "        lab = r[col]\n",
    "        clap_scores[lab][docid] = max(clap_scores[lab].get(docid, 0), score)\n",
    "\n",
    "def rank_docs_for_label(score_dict, all_docids):\n",
    "    \"\"\"Returns a ranked list of docids. Ties broken deterministically by docid.\"\"\"\n",
    "    # score_dict: docid -> score\n",
    "    return sorted(all_docids, key=lambda d: (-score_dict.get(d, 0), d))\n",
    "\n",
    "all_docids = sorted(gold[\"filename\"].tolist())\n",
    "\n",
    "def evaluate_run(ranked, qrel_for_query, k=50):\n",
    "    rels = [qrel_for_query.get(d, 0) for d in ranked]\n",
    "    ideal = sorted(qrel_for_query.values(), reverse=True)\n",
    "\n",
    "    ndcg = ndcg_at_k(rels, ideal, k)\n",
    "\n",
    "    binary = [r > 0 for r in rels]\n",
    "    ap = ap_at_k(binary, k)\n",
    "    rec = recall_at_k(binary, total_relevant=len(qrel_for_query), k=k)\n",
    "    return ndcg, ap, rec\n",
    "\n",
    "# Run evaluation across all label queries\n",
    "K = 50\n",
    "rows = []\n",
    "for q in labels:\n",
    "    ranked = rank_docs_for_label(clap_scores[q], all_docids)\n",
    "    ndcg, ap, rec = evaluate_run(ranked, qrels[q], k=K)\n",
    "    rows.append((q, ndcg, ap, rec, len(qrels[q])))\n",
    "\n",
    "res = pd.DataFrame(rows, columns=[\"query_label\", f\"nDCG@{K}\", f\"MAP@{K}\", f\"Recall@{K}\", \"num_rel_docs\"])\n",
    "print(res.sort_values(f\"nDCG@{K}\", ascending=False).to_string(index=False))\n",
    "print(\"\\nMacro-avg (over queries):\")\n",
    "print(res[[f\"nDCG@{K}\", f\"MAP@{K}\", f\"Recall@{K}\"]].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_run(all_docids, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ranked = all_docids.copy()\n",
    "    rng.shuffle(ranked)\n",
    "    return ranked\n",
    "\n",
    "K = 50\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "rand_rows = []\n",
    "\n",
    "for seed in seeds:\n",
    "    per_q = []\n",
    "    for q in labels:\n",
    "        ranked = random_run(all_docids, seed=seed)\n",
    "        per_q.append(evaluate_run(ranked, qrels[q], k=K))\n",
    "    per_q = np.array(per_q)  # shape (num_queries, 3)\n",
    "    rand_rows.append((seed, per_q[:,0].mean(), per_q[:,1].mean(), per_q[:,2].mean()))\n",
    "\n",
    "rand_df = pd.DataFrame(rand_rows, columns=[\"seed\", f\"nDCG@{K}\", f\"MAP@{K}\", f\"Recall@{K}\"])\n",
    "print(rand_df.to_string(index=False))\n",
    "print(\"\\nRandom baseline mean across seeds:\")\n",
    "print(rand_df[[f\"nDCG@{K}\", f\"MAP@{K}\", f\"Recall@{K}\"]].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_avg_metrics(df, ndcg_col, map_col, rec_col):\n",
    "    return np.array([df[ndcg_col].mean(), df[map_col].mean(), df[rec_col].mean()])\n",
    "\n",
    "def bootstrap_ci(per_query_values, n_boot=10000, alpha=0.05, seed=0):\n",
    "    \"\"\"\n",
    "    per_query_values: array shape (num_queries,) for one metric.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Q = len(per_query_values)\n",
    "    boots = []\n",
    "    for _ in range(n_boot):\n",
    "        samp = rng.choice(per_query_values, size=Q, replace=True)\n",
    "        boots.append(np.mean(samp))\n",
    "    boots = np.array(boots)\n",
    "    lo = np.quantile(boots, alpha/2)\n",
    "    hi = np.quantile(boots, 1 - alpha/2)\n",
    "    return lo, hi\n",
    "\n",
    "# Example: bootstrap CI for CLAP baseline nDCG@K\n",
    "ndcg_col = f\"nDCG@{K}\"\n",
    "lo, hi = bootstrap_ci(res[ndcg_col].values, n_boot=10000, seed=42)\n",
    "print(f\"CLAP macro {ndcg_col}: {res[ndcg_col].mean():.4f}  CI95 [{lo:.4f}, {hi:.4f}]\")\n",
    "\n",
    "# Paired randomization-style test (paired over queries) between CLAP and Random(mean over seeds)\n",
    "# We'll compare CLAP vs one random seed here; you can extend to avg of seeds.\n",
    "seed = 0\n",
    "rand_perq = []\n",
    "for q in labels:\n",
    "    ranked = random_run(all_docids, seed=seed)\n",
    "    ndcg, ap, rec = evaluate_run(ranked, qrels[q], k=K)\n",
    "    rand_perq.append((q, ndcg, ap, rec))\n",
    "rand_perq = pd.DataFrame(rand_perq, columns=[\"query_label\", f\"nDCG@{K}\", f\"MAP@{K}\", f\"Recall@{K}\"])\n",
    "\n",
    "diff = res.sort_values(\"query_label\")[ndcg_col].values - rand_perq.sort_values(\"query_label\")[ndcg_col].values\n",
    "print(\"Per-query nDCG diffs (CLAP - Random):\", diff.round(4))\n",
    "\n",
    "# Simple paired t-test over queries (ok as a quick check; with 8 queries, interpret cautiously)\n",
    "from math import sqrt\n",
    "mean_diff = diff.mean()\n",
    "std_diff = diff.std(ddof=1)\n",
    "t = mean_diff / (std_diff / sqrt(len(diff))) if std_diff > 0 else float(\"inf\")\n",
    "print(f\"Paired t statistic over queries (nDCG@{K}): t={t:.3f} (df={len(diff)-1})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc986c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
